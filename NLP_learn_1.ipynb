{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.lang.en import English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text='There is no way the self-driving cars are ever gonna get far and take on the majority of cars on the roads. It aint gonna be no success. Cause people are never gonna go for it. A hell of a lot people actually really like driving,most of them.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There is no way the self-driving cars are ever gonna get far and take on the majority of cars on the roads.',\n",
       " 'It aint gonna be no success.',\n",
       " 'Cause people are never gonna go for it.',\n",
       " 'A hell of a lot people actually really like driving,most of them.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sent_tokens=sent_tokenize(example_text)\n",
    "Word_tokens=(word_tokenize(example_text))\n",
    "print(Word_tokens[6])\n",
    "\n",
    "from collections import Counter\n",
    "import regex as re\n",
    "counts = Counter()\n",
    "words = re.compile(r'\\w+')\n",
    "\n",
    "for sentence in example_text:\n",
    "    counts.update(words.findall(sentence.lower()))\n",
    "\n",
    "Sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x10d6107d0>\n",
      "[u'There', u'is', u'no', u'way', u'the', u'self', u'-', u'driving', u'cars', u'are', u'coming', u'soon', u'.']\n"
     ]
    }
   ],
   "source": [
    "example_text1=u\"There is no way the self-driving cars are coming soon.\"\n",
    "parser=English()\n",
    "print(parser)\n",
    "tokens = parser(example_text1)\n",
    "tokens = [token.orth_ for token in tokens if not token.orth_.isspace()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "morewords = ['cars', 'shipment', 'only', 'copy', 'attach', 'material']\n",
    "stop_words.update(morewords)#update the additional common words that you may want to omit\n",
    "#list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'way', 'self-driving', 'ever', 'gon', 'na', 'get', 'far', 'take', 'majority', 'roads', '.', 'It', 'aint', 'gon', 'na', 'success', '.', 'Cause', 'people', 'never', 'gon', 'na', 'go', '.', 'A', 'hell', 'lot', 'people', 'actually', 'really', 'like', 'driving', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_text=[]#After removing the common words\n",
    "\n",
    "for w in Word_tokens:# w here is not the index number but rather the word itself\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)#Hence i append w here\n",
    "#print(filtered_text)    \n",
    "\n",
    "#An alternative shorter method\n",
    "filtered_words=[w for w in Word_tokens if not w in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'there', 'is', 'no', u'way', u'the', u'self-driv', u'car', u'are', u'ever', u'gon', 'na', u'get', u'far', u'and', u'take', 'on', u'the', u'major', 'of', u'car', 'on', u'the', u'road', '.', 'it', u'aint', u'gon', 'na', 'be', 'no', u'success', '.', u'caus', u'peopl', u'are', u'never', u'gon', 'na', 'go', u'for', 'it', '.', 'a', u'hell', 'of', 'a', u'lot', u'peopl', u'actual', u'realli', u'like', u'drive', ',', u'most', 'of', u'them', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sn=SnowballStemmer('english')\n",
    "ps=PorterStemmer()\n",
    "PortStem=[]\n",
    "SnowStem=[]\n",
    "for w in Word_tokens:\n",
    "    PortStem.append(ps.stem(w))#change words to their origin by removing suffixes and affixes\n",
    "\n",
    "for w in Word_tokens:\n",
    "    SnowStem.append(sn.stem(w))\n",
    "print(SnowStem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize the Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'drive']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl=WordNetLemmatizer()\n",
    "lemma_corrections=[]\n",
    "#for w in Word_tokens:\n",
    "lemma_corrections.append(wnl.lemmatize('driving','v'))#after letter v we replace the suffix etc\n",
    "\n",
    "print(lemma_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/senthu/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "nltk.download('state_union')\n",
    "from nltk import PunktSentenceTokenizer\n",
    "\n",
    "train_text=state_union.raw(\"/Users/senthu/Downloads/TextNLTK.txt\")\n",
    "sample_text=state_union.raw(\"/Users/senthu/Downloads/TextNLTK.txt\")\n",
    "\n",
    "custom_tokenizer=PunktSentenceTokenizer(train_text)\n",
    "tokenized=custom_tokenizer.tokenize(sample_text)#matching and checking sample text based on train text\n",
    "Tag=[]\n",
    "def process_con():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            Tag.append(tagged)\n",
    "                      \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    #return Tag    \n",
    "process_con()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object is not callable\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_text=state_union.raw(\"/Users/senthu/Downloads/TextNLTK.txt\")\n",
    "sample_text=state_union.raw(\"/Users/senthu/Downloads/TextNLTK.txt\")\n",
    "\n",
    "custom_tokenizer=PunktSentenceTokenizer(train_text)\n",
    "tokenized=custom_tokenizer.tokenize(sample_text)#matching and checking sample text based on train text\n",
    "def process_con():\n",
    "    try:\n",
    "        for i in tokenized(5):\n",
    "            words=word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            \n",
    "            named_Entity=nltk.ne_chunk(tagged, binary=True)#call the named entity chunk function\n",
    "            named_Entity.draw()\n",
    "            return named_Entity.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "    \n",
    "            \n",
    "process_con()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
