{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1DTranspose(input_tensor, filters, kernel_size, strides=2, padding='same'):\n",
    "    \"\"\"\n",
    "        input_tensor: tensor, with the shape (batch_size, time_steps, dims)\n",
    "        filters: int, output dimension, i.e. the output tensor will have the shape of (batch_size, time_steps, filters)\n",
    "        kernel_size: int, size of the convolution kernel\n",
    "        strides: int, convolution step size\n",
    "        padding: 'same' | 'valid'\n",
    "    \"\"\"\n",
    "    x = layers.Lambda(lambda x: tf.keras.backend.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = layers.Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding,data_format='channels_last')(x)\n",
    "    x = layers.Lambda(lambda x: tf.keras.backend.squeeze(x, axis=2))(x)\n",
    "    return x\n",
    "\n",
    "def buildDeepEncoderDecoder(subSignalLength):\n",
    "    knSize = 15\n",
    "    filters = np.array([1,64,128,256,256,512,512,1024,2048]);\n",
    "    inp = layers.Input(shape=(subSignalLength,1))\n",
    "    # Encoder convolution layer 1\n",
    "    enConvL1= layers.Conv1D(filters = filters[1], kernel_size = knSize,padding='same')(inp)\n",
    "    print(\"enConvL1\", enConvL1)\n",
    "    # Add activation function\n",
    "    enConvL1Af = layers.Activation(tf.nn.relu)(enConvL1)\n",
    "    del enConvL1\n",
    "    enConvL1AfMP = layers.MaxPooling1D(2)(enConvL1Af)\n",
    "    del enConvL1Af\n",
    "    \n",
    "    # Encoder convolution layer 2\n",
    "    enConvL2= layers.Conv1D(filters = filters[2], kernel_size = knSize,padding='same')(enConvL1AfMP)\n",
    "    print(\"enConvL2\", enConvL2)\n",
    "    del enConvL1AfMP\n",
    "    # Add activation function\n",
    "    enConvL2Af = layers.Activation(tf.nn.relu)(enConvL2)\n",
    "    del enConvL2\n",
    "    enConvL2AfMP = layers.MaxPooling1D(2)(enConvL2Af)\n",
    "    del enConvL2Af \n",
    "    \n",
    "    # Encoder convolution layer 3\n",
    "    enConvL3= layers.Conv1D(filters = filters[3], kernel_size = knSize,padding='same')(enConvL2AfMP)\n",
    "    print(\"enConvL3\", enConvL3)\n",
    "    del enConvL2AfMP \n",
    "    # Add activation function\n",
    "    enConvL3Af = layers.Activation(tf.nn.relu)(enConvL3)\n",
    "    del enConvL3\n",
    "    enConvL3AfMP = layers.MaxPooling1D(2)(enConvL3Af)\n",
    "    del enConvL3Af\n",
    "    \n",
    "    # Encoder convolution layer 4\n",
    "    enConvL4= layers.Conv1D(filters = filters[4], kernel_size = knSize,padding='same')(enConvL3AfMP)\n",
    "    print(\"enConvL4\",enConvL4)\n",
    "    del enConvL3AfMP\n",
    "    # Add activation function\n",
    "    enConvL4Af = layers.Activation(tf.nn.relu)(enConvL4)\n",
    "    # del enConvL4\n",
    "    enConvL4AfMP = layers.MaxPooling1D(2)(enConvL4Af)\n",
    "    del enConvL4Af\n",
    "    \n",
    "    # Encoder convolution layer 5\n",
    "    enConvL5= layers.Conv1D(filters = filters[5], kernel_size = knSize,padding='same')(enConvL4AfMP)\n",
    "    print(\"enConvL5\",enConvL5)\n",
    "    del enConvL4AfMP\n",
    "    # Add activation function\n",
    "    enConvL5Af = layers.Activation(tf.nn.relu)(enConvL5)\n",
    "    del enConvL5\n",
    "    enConvL5AfMP = layers.MaxPooling1D(2)(enConvL5Af)\n",
    "    del enConvL5Af\n",
    "    \n",
    "    # Encoder convolution layer 6\n",
    "    enConvL6= layers.Conv1D(filters = filters[6], kernel_size = knSize,padding='same')(enConvL5AfMP)\n",
    "    print(\"enConvL6\",enConvL6)\n",
    "    del enConvL5AfMP\n",
    "    # Add activation function\n",
    "    enConvL6Af = layers.Activation(tf.nn.relu)(enConvL6)\n",
    "    del enConvL6\n",
    "    enConvL6AfMP = layers.MaxPooling1D(2)(enConvL6Af)\n",
    "    del enConvL6Af\n",
    "    \n",
    "    # Encoder convolution layer 7\n",
    "    enConvL7= layers.Conv1D(filters = filters[7], kernel_size = knSize,padding='same')(enConvL6AfMP)\n",
    "    print(\"enConvL7\",enConvL7)\n",
    "    del enConvL6AfMP\n",
    "    # Add activation function\n",
    "    enConvL7Af = layers.Activation(tf.nn.relu)(enConvL7)\n",
    "    del enConvL7\n",
    "    enConvL7AfMP = layers.MaxPooling1D(2)(enConvL7Af)\n",
    "    del enConvL7Af\n",
    "    \n",
    "    # Encoder convolution layer 8\n",
    "    enConvL8= layers.Conv1D(filters = filters[8], kernel_size = knSize,padding='same')(enConvL7AfMP)\n",
    "    print(\"enConvL8\", enConvL8)\n",
    "    del enConvL7AfMP\n",
    "    # Add activation function\n",
    "    enConvL8Af = layers.Activation(tf.nn.relu)(enConvL8)\n",
    "    del enConvL8\n",
    "    enConvL8AfMP = layers.MaxPooling1D(2)(enConvL8Af)\n",
    "    print(\"enConvL8AfMP\",enConvL8AfMP)\n",
    "    \n",
    "    \n",
    "    # Decoder convolution transpose layer 1\n",
    "    deConvL1= conv1DTranspose(enConvL8AfMP,filters = filters[7], kernel_size = knSize)\n",
    "    print(\"deConvL1\", deConvL1)\n",
    "    del enConvL8Af\n",
    "    # Add activation function\n",
    "    deConvL1Af = layers.Activation(tf.nn.relu)(deConvL1)\n",
    "    del deConvL1\n",
    "    # Decoder convolution layer 2\n",
    "    deConvL2= conv1DTranspose(deConvL1Af,filters = filters[6], kernel_size = knSize)\n",
    "    print(\"deConvL2\", deConvL2)\n",
    "    del deConvL1Af\n",
    "    # Add activation function\n",
    "    deConvL2Af = layers.Activation(tf.nn.relu)(deConvL2)\n",
    "    del deConvL2\n",
    "    # Decoder convolution layer 3\n",
    "    deConvL3= conv1DTranspose(deConvL2Af,filters = filters[5], kernel_size = knSize)\n",
    "    print(\"deConvL3\", deConvL3)\n",
    "    del deConvL2Af\n",
    "    # Add activation function\n",
    "    deConvL3Af = layers.Activation(tf.nn.relu)(deConvL3)\n",
    "    del deConvL3\n",
    "    # Decoder convolution layer 4\n",
    "    deConvL4= conv1DTranspose(deConvL3Af,filters = filters[4], kernel_size = knSize)\n",
    "    print(\"deConvL4\", deConvL4)\n",
    "    del deConvL3Af\n",
    "    # Add activation function\n",
    "    deConvL4Af = layers.Activation(tf.nn.relu)(deConvL4)\n",
    "    del deConvL4\n",
    "    # Decoder convolution layer 5\n",
    "    deConvL5= conv1DTranspose(deConvL4Af,filters = filters[3], kernel_size = knSize)\n",
    "    print(\"deConvL5\", deConvL5)\n",
    "    del deConvL4Af\n",
    "    skipConnection2 = layers.add([enConvL4,deConvL5])\n",
    "    # Add activation function\n",
    "    deConvL5Af = layers.Activation(tf.nn.relu)(skipConnection2)\n",
    "    del skipConnection2\n",
    "    del deConvL5\n",
    "    # Decoder convolution layer 6\n",
    "    deConvL6= conv1DTranspose(deConvL5Af,filters = filters[2], kernel_size = knSize)\n",
    "    print(\"deConvL6\", deConvL6)\n",
    "    del deConvL5Af\n",
    "    # skip connection2\n",
    "    # skipConnection2 = layers.add([enConvL2, deConvL6])\n",
    "    # Add activation function\n",
    "    deConvL6Af = layers.Activation(tf.nn.relu)(deConvL6)\n",
    "    del deConvL6\n",
    "    # Decoder convolution layer 7\n",
    "    deConvL7= conv1DTranspose(deConvL6Af,filters = filters[1], kernel_size = knSize, padding='same')\n",
    "    print(\"deConvL7\", deConvL7)\n",
    "    del deConvL6Af\n",
    "    # Add activation function\n",
    "    deConvL7Af = layers.Activation(tf.nn.relu)(deConvL7)\n",
    "    del deConvL7\n",
    "    # Decoder convolution layer 8\n",
    "    deConvL8= conv1DTranspose(deConvL7Af,filters = filters[0], kernel_size = knSize)\n",
    "    print(\"deConvL8\", deConvL8)\n",
    "    # del deConvL7Af\n",
    "    skipConnection1 = layers.add([inp, deConvL8])\n",
    "    # Add activation function\n",
    "    deConvL8Af = layers.Activation(tf.nn.relu)(skipConnection1)\n",
    "    return inp, deConvL8Af\n",
    "\n",
    "def loadData(segmentLen):\n",
    "    trainOutput = []\n",
    "    groundTruthOutput = []\n",
    "    testingOutput = []\n",
    "    fileNameList = glob.glob('./data/training/*.csv')\n",
    "    fileNameGTList = glob.glob('./data/ground_truth/*.csv')\n",
    "    fileNameTestingList = glob.glob('./data/testing/*.csv')\n",
    "    #-----------load training data------------------------\n",
    "    for filename in fileNameList:\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        fEcgFile = open(filename)\n",
    "        #reading the File with the help of csv.reader()\n",
    "        fEcgReader = csv.reader(fEcgFile)\n",
    "        #storing the values contained in the Reader into Data\n",
    "        fEcgData = list(fEcgReader)\n",
    "        #printing the each line of the Data in the console\n",
    "        for data in fEcgData:\n",
    "            segNum = int(len(data)/segmentLen)\n",
    "            for n in range(segNum):\n",
    "                segDataTmp = [float(x) for x in data[n*segmentLen:(n+1)*segmentLen]]\n",
    "                #normalize abs(segDataTmp) to range[0,1]\n",
    "                minSD = min(segDataTmp)\n",
    "                maxSD = max(segDataTmp)\n",
    "                rangeSD = maxSD-minSD\n",
    "                segDataAbsNorm = [(x-minSD)/rangeSD for x in segDataTmp]\n",
    "                segData = np.array([segDataAbsNorm]).T\n",
    "                trainOutput.append(segData)\n",
    "        fEcgFile.close()\n",
    "    trainOutput = np.array(trainOutput)\n",
    "    #-----------load ground truth data------------------------\n",
    "    for filename in fileNameGTList:\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        fEcgFile = open(filename)\n",
    "        #reading the File with the help of csv.reader()\n",
    "        fEcgReader = csv.reader(fEcgFile)\n",
    "        #storing the values contained in the Reader into Data\n",
    "        fEcgData = list(fEcgReader)\n",
    "        #printing the each line of the Data in the console\n",
    "        for data in fEcgData:\n",
    "            segNum = int(len(data)/segmentLen)\n",
    "            for n in range(segNum):\n",
    "                segDataTmp = [float(x) for x in data[n*segmentLen:(n+1)*segmentLena]]\n",
    "                #normalize abs(segDataTmp) to range[0,1]\n",
    "                minSD = min(segDataTmp)\n",
    "                maxSD = max(segDataTmp)\n",
    "                rangeSD = maxSD-minSD\n",
    "                segDataAbsNorm = [(x-minSD)/rangeSD for x in segDataTmp]\n",
    "                segData = np.array([segDataAbsNorm]).T\n",
    "                groundTruthOutput.append(segData)\n",
    "        fEcgFile.close()\n",
    "    groundTruthOutput = np.array(groundTruthOutput)\n",
    "    #-----------load testing truth data------------------------\n",
    "    for filename in fileNameTestingList:\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        fEcgFile = open(filename)\n",
    "        #reading the File with the help of csv.reader()\n",
    "        fEcgReader = csv.reader(fEcgFile)\n",
    "        #storing the values contained in the Reader into Data\n",
    "        fEcgData = list(fEcgReader)\n",
    "        #printing the each line of the Data in the console\n",
    "        for data in fEcgData:\n",
    "            segNum = int(len(data)/segmentLen)\n",
    "            for n in range(segNum):\n",
    "                segDataTmp = [float(x) for x in data[n*segmentLen:(n+1)*segmentLen]]\n",
    "                #normalize abs(segDataTmp) to range[0,1]\n",
    "                minSD = min(segDataTmp)\n",
    "                maxSD = max(segDataTmp)\n",
    "                rangeSD = maxSD-minSD\n",
    "                segDataAbsNorm = [(x-minSD)/rangeSD for x in segDataTmp]\n",
    "                segData = np.array([segDataAbsNorm]).T\n",
    "                testingOutput.append(segData)\n",
    "        fEcgFile.close()\n",
    "    testingOutput = np.array(testingOutput)\n",
    "    return trainOutput, groundTruthOutput, testingOutput\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadChallenge2013Data(segmentLen):\n",
    "    trainOutput = []\n",
    "    groundTruthOutput = []\n",
    "    testingOutput = []\n",
    "#     fileNameList = glob.glob('./data/training/*.csv')\n",
    "#     fileNameGTList = glob.glob('./data/ground_truth/*.csv')\n",
    "    trainDir = \"./data/training/\"\n",
    "    gtDir = \"./data/ground_truth/\"\n",
    "    fileNameList = os.listdir(trainDir)\n",
    "    print(fileNameList)\n",
    "    #-----------load training data------------------------\n",
    "    for filename in fileNameList:\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        fEcgFile = open(trainDir + filename)\n",
    "        #reading the File with the help of csv.reader()\n",
    "        fEcgReader = csv.reader(fEcgFile)\n",
    "        print(fEcgReader)\n",
    "        #storing the values contained in the Reader into Data\n",
    "        fEcgData = list(fEcgReader)\n",
    "        #printing the each line of the Data in the console\n",
    "        for data in fEcgData:\n",
    "            segNum = int(len(data)/segmentLen)\n",
    "            for n in range(segNum):\n",
    "                segDataTmp = [np.abs(float(x)) for x in data[n*segmentLen:(n+1)*segmentLen]]\n",
    "                #normalize abs(segDataTmp) to range[0,1]\n",
    "                minSD = min(segDataTmp)\n",
    "                maxSD = max(segDataTmp)\n",
    "                rangeSD = maxSD-minSD\n",
    "                segDataAbsNorm = [(x-minSD)/rangeSD for x in segDataTmp]\n",
    "                segData = np.array([segDataAbsNorm]).T\n",
    "                trainOutput.append(segData)\n",
    "        fEcgFile.close()\n",
    "    trainOutput = np.array(trainOutput)\n",
    "    #-----------load ground truth data------------------------\n",
    "    for filename in fileNameList:\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        filenameSplit = filename.split(\".\")\n",
    "        #openning the csv file which is in the same location of this python file\n",
    "        fEcgFile = open(gtDir+filenameSplit[0]+\"_gt.\"+filenameSplit[1])\n",
    "        #reading the File with the help of csv.reader()\n",
    "        fEcgReader = csv.reader(fEcgFile)\n",
    "        #storing the values contained in the Reader into Data\n",
    "        fEcgData = list(fEcgReader)\n",
    "        #printing the each line of the Data in the console\n",
    "        for data in fEcgData:\n",
    "            segNum = int(len(data)/segmentLen)\n",
    "            for n in range(segNum):\n",
    "                segDataTmp = [np.abs(float(x)) for x in data[n*segmentLen:(n+1)*segmentLen]]\n",
    "                #normalize abs(segDataTmp) to range[0,1]\n",
    "                minSD = min(segDataTmp)\n",
    "                maxSD = max(segDataTmp)\n",
    "                rangeSD = maxSD-minSD\n",
    "                segDataAbsNorm = [(x-minSD)/rangeSD for x in segDataTmp]\n",
    "                segData = np.array([segDataAbsNorm]).T\n",
    "                groundTruthOutput.append(segData)\n",
    "        fEcgFile.close()\n",
    "    groundTruthOutput = np.array(groundTruthOutput)\n",
    "    return trainOutput, groundTruthOutput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'a01.csv']\n",
      "<_csv.reader object at 0x13cee8198>\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 306: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-139ca7782393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msegmentLen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1792\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroundTruthOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadChallenge2013Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildDeepEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstructed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cf1544ac4f3b>\u001b[0m in \u001b[0;36mloadChallenge2013Data\u001b[0;34m(segmentLen)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfEcgReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#storing the values contained in the Reader into Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfEcgData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfEcgReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#printing the each line of the Data in the console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfEcgData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 306: invalid start byte"
     ]
    }
   ],
   "source": [
    "segmentLen = 1792\n",
    "trainOutput, groundTruthOutput = loadChallenge2013Data(segmentLen)\n",
    "\n",
    "inp, reconstructed = buildDeepEncoderDecoder(segmentLen)\n",
    "encoder = models.Model(inputs = inp, outputs = reconstructed)\n",
    "encoder.compile(optimizer='adam',\\\n",
    "              loss=tf.keras.metrics.mean_squared_error,\\\n",
    "              metrics=['accuracy'])\n",
    "encoder.fit(x=trainOutput, y=groundTruthOutput,batch_size=64, epochs=10)\n",
    "\n",
    "testingData = trainOutput[400:401]\n",
    "gtData = groundTruthOutput[400:401]\n",
    "outputData = new_model.predict(testingData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
